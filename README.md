# ğŸ§  AvicennaFlow: Biologically Plausible Learning in CNNs

This repository contains the official codebase for the project:

**"Biologically Plausible Learning in CNNs: A Comparison of Backpropagation, Feedback Alignment, and Kolenâ€“Pollack Algorithms for Image Classification"**

ğŸ“š **Conducted as part of the [Neuromatch Academy NeuroAI Program](https://neuroai.neuromatch.io/), July 2025.**

---

## ğŸ§¬ Project Summary

AvicennaFlow investigates **biologically plausible learning rules** in convolutional neural networks, comparing:

- **Backpropagation (BP)**
- **Feedback Alignment (DFA)**
- **Kolenâ€“Pollack Algorithm (DKP)**

We analyze these algorithms on **MNIST** and **CIFAR-10** datasets using multiple metrics:
- Accuracy and training dynamics  
- Gradient signal-to-noise ratio (SNR)  
- Cosine similarity of gradients  
- Representational similarity (RSA & RDMs)  
- Robustness to adversarial attacks (FGSM)

---

## âš™ï¸ Setup

1. Install dependencies:

```bash
pip install -r requirements.txt
````

2. Run the notebook:

```bash
jupyter notebook AvicennaFlow_Project.ipynb
```

> âš ï¸ Run cells in order for correct setup and training.

---


## ğŸ§ª Evaluation Methods

* ğŸ”¬ **RDMs & RSA**: Compare internal representations layer-wise.
* ğŸ“Š **Cosine Similarity**: Between BP and DKP/DFA gradients.
* ğŸ¯ **Adversarial Accuracy**: Under FGSM attacks.
* ğŸŒ€ **t-SNE Visualization**: Feature space separation.
* ğŸ“‰ **Gradient SNR**: Assess clarity of learning signal.

---

## ğŸ–¼ï¸ Sample Visualizations

Key results are stored in the [`plots/`](./plots) directory, including:

- RDM analyses (CIFAR-10 & MNIST)  
- Representational paths  
- Gradient SNR comparisons  
- t-SNE embeddings  
- Adversarial accuracy plots
